# Gradient-Checking
In this project, I have performed 'Gradient Checking' to check whether the backpropogation is working fine.


1) What is gradient checking and How does gradient checking work?

Backpropagation computes the gradients  âˆ‚ğ½âˆ‚ğœƒ , where  ğœƒ  denotes the parameters of the model.  ğ½  is computed using forward propagation and your loss function.

Because forward propagation is relatively easy to implement, you're confident you got that right, and so you're almost 100% sure that you're computing the cost  ğ½  correctly. Thus, you can use your code for computing  ğ½  to verify the code for computing  âˆ‚ğ½âˆ‚ğœƒ .

Let's look back at the definition of a derivative (or gradient):
âˆ‚ğ½âˆ‚ğœƒ=limğœ€â†’0ğ½(ğœƒ+ğœ€)âˆ’ğ½(ğœƒâˆ’ğœ€)2ğœ€(1)

If you're not familiar with the " limğœ€â†’0 " notation, it's just a way of saying "when  ğœ€  is really really small."

We know the following:

âˆ‚ğ½âˆ‚ğœƒ  is what you want to make sure you're computing correctly.

You can compute  ğ½(ğœƒ+ğœ€)  and  ğ½(ğœƒâˆ’ğœ€)  (in the case that  ğœƒ  is a real number), since you're confident your implementation for  ğ½  is correct.
Lets use equation (1) and a small value for  ğœ€  to convince your CEO that your code for computing  âˆ‚ğ½âˆ‚ğœƒ  is correct!
